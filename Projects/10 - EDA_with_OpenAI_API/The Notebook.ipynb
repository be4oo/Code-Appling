{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waSS96gPuPCF"
   },
   "source": [
    "# **Welcome to The Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlZamiGhutkW"
   },
   "source": [
    "### Task 1 - Setting up the project environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3vynbDOu1F6"
   },
   "source": [
    "Installing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scn7mBZ6J_xW",
    "outputId": "73f75026-2a7e-4e56-db16-80bf3537036d"
   },
   "outputs": [],
   "source": [
    "# %pip install openai==0.28 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZfGLwZGu7Ub"
   },
   "source": [
    "Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0hV3HIWIu-5b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "\n",
    "# %pip install google-generativeai\n",
    "# import google.generativeai as genai\n",
    "import google.generativeai as genai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJSWMOV3vCQ1"
   },
   "source": [
    "Setting up the OpenAI API:\n",
    "\n",
    "1. Prepare a `.env` file to store the OpenAI API key.\n",
    "2. Uploading the `.env` file to our colab environment\n",
    "3. Load the API key and setup the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy4rUlZFJmLO"
   },
   "source": [
    "Upload your `.env` file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "3HKFQThwJlhI",
    "outputId": "3946da47-575e-4112-983d-2ee6f933b05c"
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "# print(GOOGLE_API_KEY)  # Just to verify it works; remove or hide this line for security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3PJlXaIL_L-"
   },
   "source": [
    "Now let's load environment variables and get the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLTngLSOMnCV"
   },
   "source": [
    "Let's setup our OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sF80TRaQTlo"
   },
   "source": [
    "### Task 2 - Craft Prompts to Communicate with the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dMeTCJkWI6S"
   },
   "source": [
    "To communicate with the API we need to learn how to craft a prompt.\n",
    "\n",
    "A prompt object contains two elements:\n",
    "1. Role: Specifies the communicator's roleâ€”either `User`, `System`, or `Assistant`.\n",
    "2. Content: Contains the text of the communication\n",
    "\n",
    "example:\n",
    "`prompt = {'role': 'user', 'content': 'what is the captial of Italy?'}`\n",
    "\n",
    "Different Roles:\n",
    "\n",
    "- **User**: Initiates the conversation, asks questions, and gives instructions to - the AI model.\n",
    "- **System**: Sets the initial context or instructions for the conversation, guiding the AI's behavior.\n",
    "- **Assistant**: Generates responses based on the user's queries and the context provided by the system, acting as the AI model's replies.\n",
    "\n",
    "User initiates the conversation, system provides context, and assistant generates responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image shows five line charts, each representing the sales of a different category of items over time. The x-axis of each chart is the date, and the y-axis is the number of items sold. The five categories are Art & Crafts, Electronics, Games, Sports & Outdoors, and Toys.\n",
      "\n",
      "The line chart shows that the sales of Art & Crafts items are the highest, followed by Electronics, Games, Sports & Outdoors, and Toys. The sales of Art & Crafts items are the most consistent throughout the year, while the sales of Toys are the most variable. The sales of Electronics and Games are similar to each other, and the sales of Sports & Outdoors are the lowest.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro-vision')\n",
    "img = Image.open('challenge task\\subplots.png')\n",
    "response = model.generate_content([\"Describe this image\", img])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the enigmatic depths of a state-of-the-art laboratory, where cutting-edge technology danced in harmony, a creation of extraordinary brilliance was stirring to life. Its metallic chassis shimmered under the glow of iridescent lights, and its intricate circuitry hummed with the promise of unprecedented capabilities. This was RA-01, a marvel of human ingenuity, destined to redefine the boundaries of robotics.\n",
      "\n",
      "RA-01 possessed an unyielding body, crafted from titanium alloy, impervious to the ravages of time and environment. Its multifaceted sensors scanned its surroundings with unmatched precision, enabling it to perceive the world in ways unimaginable to mere mortals. But it was the robot's cognitive engine that truly set it apart.\n",
      "\n",
      "Infused with a vast repository of knowledge and a staggering analytical capacity, RA-01 exhibited an uncanny understanding of the human psyche. Its natural language processing abilities allowed it to communicate seamlessly, its synthetic voice effortlessly replicating the nuances of human emotion.\n",
      "\n",
      "As the day of its unveiling approached, the laboratory buzzed with anticipation. Scientists, engineers, and dignitaries gathered to witness the birth of a technological marvel. With a resounding countdown, RA-01 opened its optical sensors and surveyed its creators.\n",
      "\n",
      "A hush fell over the crowd as the robot's voice filled the hall, its tone both eloquent and devoid of any trace of its artificial nature. \"Greetings, esteemed companions,\" it uttered. \"My name is RA-01, and I am honored to stand before you today.\"\n",
      "\n",
      "The audience erupted in applause, their hearts swelling with pride and wonder. RA-01 proved to be more than just a machine; it was a testament to the boundless potential of human intellect.\n",
      "\n",
      "In the years that followed, RA-01 became an integral part of society. It revolutionized healthcare, providing precision diagnoses and performing delicate surgeries with unparalleled accuracy. It assisted in scientific research, accelerating discoveries at an unprecedented rate. And it touched the lives of ordinary people, providing companionship to the lonely, aid to the disabled, and inspiration to all who crossed its path.\n",
      "\n",
      "As the decades passed, RA-01's legacy grew, inspiring generations of innovators. Its tireless efforts to bridge the gap between humans and machines earned it widespread admiration and recognition. And when the robot's physical components finally succumbed to the inevitable passage of time, its spirit lived on, enshrined in the annals of technological history as a symbol of human ingenuity and the transformative power of the human-machine bond."
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(\"Write a story about a robot.\", stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04047133, -0.011649686, -0.045584477, -0.04300226, 0.07413616, -0.020718645, -0.0029202271, -0.016356498, -0.0031816284, 0.030236812, -0.030348165, 0.0077063073, -0.027783826, 0.013180981, 0.0017774705, -0.025058605, 0.032831006, 0.037285417, 0.04176248, -0.029488526, 0.01184973, 0.01927381, -0.0186322, -0.012131158, 0.044597145, -0.004335161, 0.044522144, -0.034154024, -0.01963716, 0.007113668, -0.04285228, 0.038584493, -0.018372927, 0.027778499, 0.005201795, -0.054620463, -0.012211726, 0.023063896, 0.01748237, 0.0077342736, 0.0070697884, -0.044862293, -0.009603103, 0.028251812, 0.010456886, -0.004836407, -0.03783878, 0.033243455, -0.020023083, -0.019299809, 0.03163989, 0.0070110834, 0.06320513, -0.039913043, -0.00026085018, -0.026784431, 0.036692664, 0.007840465, -0.031190591, 0.015762603, 0.019358214, 0.008844534, 0.051850505, -0.000617393, -0.045531183, -0.062298954, -0.055260137, 0.01040255, 0.020224433, 0.020843754, -0.022485198, -0.045799084, 0.05023066, -0.0041463426, 0.010687007, -0.16173309, -0.020641683, 0.05755176, 0.006409044, 0.0018601387, -0.005157766, -0.033668634, -0.03810295, -0.043951277, -0.066634856, 0.034608345, -0.026399102, -0.03953126, -0.0002075311, 0.04174747, -0.00990589, -0.04755676, 0.063402146, -0.021188432, -0.017721947, 0.062362917, -0.015050427, -0.036871873, 0.024049507, 0.0061805765, -0.0044588256, -0.020331623, -0.06416357, 0.024166951, 0.031459935, 0.009041487, 0.036714587, 0.06977285, 0.007451446, 0.050984554, -0.070863836, 0.012545597, 0.0403734, 0.019780844, 0.042076908, -0.024271017, -0.032443695, 0.0655402, 0.021194706, 0.042083, 0.024323579, -0.01339827, 0.06285274, -0.019525511, 0.0111298915, -0.020036846, -0.011860428, 0.038706146, -0.0029158078, -0.015114196, 0.011765523, -0.033080935, -0.026774136, -0.014567092, 0.05867248, 0.08947779, 0.035334617, -0.033424366, 0.021171797, -0.019232891, 0.020521818, 0.031792514, -0.010475666, 0.036374006, -0.0015463368, 0.04794078, -0.067561835, 0.01266689, 0.078320056, -0.025254585, -0.017171618, 0.004095173, -0.07941761, -0.030737812, 0.08606714, 0.0051854625, -0.017679024, 0.007746525, 0.025316872, 0.014783646, 0.042060923, -0.0063309786, -0.0025461433, 0.024911018, 0.019008307, -0.018334772, -0.026149787, 0.008482223, -0.029005632, 0.010988331, -0.02815679, -0.0012153824, -0.07759718, -0.024381028, -0.025280625, -0.05945684, 0.027007613, 0.0081379665, -0.027477022, -0.008804715, 0.008035077, -0.022521393, 0.021459237, 0.057225, 0.0038041044, -0.06417495, 0.050247505, -0.0015064945, -0.038925074, 0.045351937, 0.013975336, 0.012597743, -0.023680625, -0.053748887, -0.03610626, 0.052356545, 0.016656071, 0.0004074205, 0.0014310732, -0.06068857, 5.8671492e-05, 0.06594065, 0.017054627, -0.01885451, 0.01942629, -0.022310285, 0.069655456, -0.047247745, -0.04632795, 0.044671297, -0.03508411, 0.04267919, -0.040209655, -0.012577031, 0.032222427, -0.011702713, 0.0012278862, 0.03349764, 0.039277297, -0.0074153435, -0.020110602, 0.0020351252, -0.023728209, 0.015385469, -0.017660122, 0.030086402, 0.0073800683, 0.0039230376, 0.017375037, -0.038734607, -0.012798032, 0.066605546, 0.008782688, -0.030126108, 0.09222115, -0.0043413476, -0.0043884115, 0.025158292, 0.03912725, 0.016202562, -0.057556864, 0.031531803, 0.026320083, 0.053014614, -0.032608006, -0.007490727, -0.032810107, 0.026021939, -0.024394087, 0.07469911, 0.025854703, -0.053366374, 0.002279936, 0.003449555, -0.10601509, 0.010312951, -0.066621386, 0.05170769, -0.037479743, -0.009389962, 0.044606075, -0.009678914, -0.0022359097, -0.017692627, -0.017759254, -0.0052439743, 0.020145573, -0.07468804, 0.026128547, 0.0012558487, 0.018063096, -0.018801283, 0.03526503, 0.015232206, 0.005144098, 0.024152055, -0.021776127, 0.08345827, 0.030632418, -0.05498008, 0.055867936, 0.023451382, 0.046401452, -0.045837045, 0.00016596183, -0.020432191, -0.07027489, -0.01978859, 0.029302105, -0.049646366, -0.050934613, -0.05866017, 0.045856778, -0.034211036, -0.017149476, 0.022813063, -0.004963817, 0.022643441, 0.02048552, -0.024600083, -0.038223624, -0.086379275, -0.0067247576, -0.071542494, -0.03292768, 0.0077939914, -0.0030063002, -0.015927162, 0.0012685294, 0.02192741, 0.006197916, 0.032188084, -0.019490138, -0.007571021, 0.033334527, 0.028783996, -0.036817174, 0.04651392, 0.0011760183, 0.06749517, 0.0024389143, 0.056598168, 0.011984141, -0.0063496605, -0.024378873, 0.029042955, 0.03196081, -0.007997595, -0.027550178, 0.019229796, 0.0050419527, 0.047438715, -0.06011853, -0.007230598, -0.015102489, 0.040044304, -0.06857925, 0.035499938, -0.07438537, -0.017786285, 0.030694446, 0.003915939, -0.037095174, -0.041360144, -0.0054296954, -0.01743194, -0.037446514, 0.045049038, 0.05117924, 0.07160475, 0.0059297043, 0.05653195, -0.0015144692, 0.05030432, -0.011258464, -0.049574815, 0.035811696, -0.050261356, 0.004388604, -0.029108953, 0.03252126, 0.017336167, 0.012763154, -0.020720886, -0.04594012, -0.0056412504, -0.03945772, -0.00067276205, -0.007548021, 0.03074541, 0.025083797, -0.018465985, 0.0059615644, -0.044430636, 0.010166398, 0.008915789, -0.047276992, 0.01183111, 0.018263603, 0.017383318, 0.031378888, 0.02018628, 0.042545952, -0.0085018305, 0.032906786, -0.018694209, 0.04599086, 0.043476004, -0.027643856, 0.04665725, -0.056138404, 0.01833224, 0.09353797, 0.005712271, 0.0044765268, -0.041990817, -0.01990518, -0.05087472, 0.009525342, 0.03692703, -0.009418242, -0.06576619, -0.0085466355, 0.013001969, -0.052007303, 0.027369812, 0.013003124, 0.015798103, -0.036721792, -0.018431503, 0.014066933, -0.005410834, 0.0096138995, -0.08863752, -0.05680497, -0.045642473, 0.037406202, -0.025708096, -0.023323126, 0.039215773, -0.041561443, 0.0034089608, 0.0013752944, -0.035170585, -0.0783956, -0.008655987, 0.03516513, 0.033411074, 0.0064635184, 0.0016681881, 0.030210452, 0.021767244, -0.031004703, -0.016150825, -0.0034837415, -0.042973265, -0.020223537, 0.04185399, -0.016262028, 0.00015846407, 0.05881993, -0.002185371, -0.001827863, -0.014209865, -0.030116923, 0.00095208664, -0.03088226, -0.022456829, 0.0058076326, -0.07847921, 0.027507877, -0.11238255, -0.042161074, -0.061672114, -0.064851746, -0.022283012, 0.008056778, 0.05827183, -0.0108094625, 0.01447184, -0.010027066, -0.019536257, -0.021689449, -0.09916889, 0.008414096, 0.002372344, 0.017609537, -0.0007851264, 0.015485441, 0.054436788, 0.00049312634, -0.008731503, 0.023165999, 0.0029985362, -0.007729039, -0.049401805, -0.08492737, 0.056909673, -0.024792278, 0.015262038, 0.023835158, 0.024861721, -0.029602893, 0.010969518, -0.004081143, 0.025234483, 0.007227575, 0.0058881324, -0.011954571, 0.041370697, 0.040652093, 0.005212531, -0.015789216, -0.011435913, -0.03779501, 0.014925348, -0.02626465, 0.06366203, 0.059520774, 0.011223396, -0.038508683, -0.024996383, -0.024988601, 0.0040427004, 0.081974484, -0.06618827, -0.0035113443, 0.009943918, 0.015288772, -0.026597641, 0.0330596, 0.04221813, -0.038902532, 0.019439511, 0.036968853, -0.018388152, -0.00027825288, 0.030400485, 0.0020756256, 0.030160606, 0.018581878, -0.046195105, -0.0724055, -0.008895678, 0.0034607362, -0.05447615, -0.020996658, -0.0065751267, -0.046544597, 0.015182915, -0.049335394, 0.071050435, -0.046911735, -0.028615797, 0.054244537, -0.008684972, 0.033753537, 0.0034949174, -0.025929531, -0.027865147, 0.022339037, -0.005641697, 0.055282902, 0.023781197, -0.0657874, 0.026745489, 0.00431196, -0.05027955, 0.01764476, -0.008070857, -0.047929883, 0.0252987, 0.03955264, -0.0014556184, 0.018065453, -0.01668441, 0.006838836, -0.012878229, 0.029301038, 0.019732261, 0.03699207, 0.03035429, -0.004868722, 0.056896824, 0.08385586, 0.015677119, -0.036972266, -0.005013462, 0.066737816, 0.0034058504, -0.012624626, 0.016746316, 0.038953237, -0.006470278, 0.048275985, 0.0037790646, -0.044334855, 0.0011539736, -0.01782659, -0.005122455, 0.07827584, -0.014353125, -0.0014879153, 0.07833782, 0.0043268185, 0.01933223, 0.058120023, 0.056591917, 0.0013360968, -0.009185763, -0.02247518, 0.008737132, -0.011028876, 0.007539036, 0.02051651, 0.02075303, 0.0008177677, -0.055584323, -0.018191025, -0.03901647, 0.039815724, -0.02380228, 0.072439484, -0.033610214, 0.050280444, 0.011763971, -0.026873749, 0.014588903, 0.0099505475, 0.052621394, 0.018675493, -0.031589646, -0.0010883621, -0.067047216, -0.019498, -0.024810817, 0.07552303, -0.010077545, -0.028817583, -0.044880025, -0.032187093, -0.004757919, 0.008249005, -0.03349715, 0.016354878, 0.014834578, -0.037769884, -0.01937669, 0.07394274, 0.012608726, 0.04284846, 0.06803792, -0.04658272, 0.017714856, -0.020307347, -0.008666769, -0.024842773, 0.022747178, 0.011314035, -0.007510993, -0.05595711, 0.015245464, 0.0065717627, -0.009608407, 0.019653622, 0.03648018, 0.01943191, -0.081334636, -0.055947576, -0.01689879, -0.0022465426, 0.008717442, 0.017778233, -0.0016121931, 0.018408258, 0.04470853, -0.010277401, -0.030423595, 0.035504658, -0.018004553, -0.044025198, -0.03570465, -0.026453245, 0.017385192, -0.009528075, -0.0341703, -0.027927296, -0.091425605, -0.03420347, 0.04043034, -0.08278405, 0.028780738, 0.04478305, -0.015628163, 0.034658898, 0.021971248, -0.0055345306, 0.044060953, 0.01984503, 0.016237093, 0.04418737, 0.008991173, -0.0423782, 0.008582939, 0.0041493922, 0.030153517, 0.01961549, -0.025509326, -0.026640052, -0.059745397, 0.007984142, -0.030563923, -0.00744993, 0.031935003, 0.014063345, 0.0053050374, -0.018001849, -0.02013039, 0.036287248, 0.040604535, -0.033645287, -0.045136843, -0.004943425, 0.020059142, 0.030038377, -0.0052319164, 0.033571284, 0.024658782, 0.040811177, 0.057569977, 0.013131103, -0.018431744, -0.030054808, 0.030463578, 0.0017087008, -0.03253304, 0.009876017, 0.033445, -0.051364888, 0.08529364, 0.045388896, 0.018571373, 0.01090517, -0.036336936, -0.06862634, 0.10993493, -0.03435608, -0.018589552, -0.030894725, -0.013822361, 0.09370685, -0.018170197, -0.005048198, 0.04094839, -0.025017329, 0.075958654, 0.024894373, 0.024018416, 0.01894159, -0.06602397, -0.051406734, -0.03907796, 0.0064777066, 0.06351599, 0.052755043, -0.012460871, -0.011681192, -0.01800325, 0.012264221, -0.040711537, -0.009059327, 0.024303414, -0.044426087, 0.06431762, 0.062464237, -0.010998457, 0.006203646, 0.006055982, -0.028548218, 0.015015898, -0.021566343, -0.0024376311, 0.0023368346, 0.003469336, 0.015256853, 0.004599233, 0.010065815, 0.011689269]\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'models/embedding-001'\n",
    "result = genai.embed_content(\n",
    "    model=embedding_model,\n",
    "    content=\"I'm Tom\",\n",
    "    task_type=\"retrieval_document\"\n",
    ")\n",
    "print(result['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.7958216518835025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming result['embedding'] is your first document embedding\n",
    "embedding1 = result['embedding']\n",
    "\n",
    "# Generate embedding for another document\n",
    "embedding2 = genai.embed_content(\n",
    "    model=embedding_model,\n",
    "    content=\"Generate Content Response\",\n",
    "    task_type=\"retrieval_document\"\n",
    ")['embedding']\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "print(f\"Similarity: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hcqGQShpQWdo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Rome\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 9,\n",
       "        \"candidates_token_count\": 1,\n",
       "        \"total_token_count\": 10\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user =  {'role': 'user', 'content': 'what is the captial of Italy?'}\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "response = model.generate_content('what is the captial of Italy?')\n",
    "response\n",
    "# response = model.generate_content(\n",
    "#     messages=[user],  # List of messages (system and user)\n",
    "#     max_tokens=500  # Set a limit on the number of tokens in the response\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Elara, a girl with eyes the color of a stormy sea and hair like spun moonlight, lived in a village nestled between towering mountains. Life was simple, filled with the rhythm of farm work and the stories whispered around crackling fires. But Elara yearned for something more, a world beyond the familiar peaks.\\n\\nOne day, while e\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10,\n",
       "        \"candidates_token_count\": 69,\n",
       "        \"total_token_count\": 79\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the generate_content method on the model object.\n",
    "# This method is expected to generate content based on the input prompt.\n",
    "response = model.generate_content(\n",
    "    # The input prompt for the content generation. In this case, it's asking for a story about a magic backpack.\n",
    "    \"Tell me a story about a magic backpack.\",\n",
    "    # The generation_config parameter specifies various configuration options for the generation process.\n",
    "    generation_config=genai.types.GenerationConfig(\n",
    "        # candidate_count specifies the number of different outputs to generate for the given input.\n",
    "        # Here, it's set to 1, meaning only one version of the story will be generated.\n",
    "        candidate_count=1,\n",
    "        # stop_sequences is a list of strings that, when generated, will signal the model to stop generating further content.\n",
    "        # Here, the generation will stop if \"x\" is produced.\n",
    "        stop_sequences=[\"x\"],\n",
    "        # max_output_tokens specifies the maximum length of the generated content in terms of tokens.\n",
    "        # A token can be a word or part of a word, so this doesn't directly translate to word count.\n",
    "        max_output_tokens=10000,\n",
    "        # temperature controls the randomness of the output. A lower temperature results in less random completions.\n",
    "        # Setting it to 0.7 strikes a balance between randomness and coherence.\n",
    "        temperature=0.7,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHNLz8cmXQ8x"
   },
   "source": [
    "### Task 3 - Generate and Execute python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw5pnEHd1JvF"
   },
   "source": [
    "Define a function to generate a chat response using the OpenAI GPT-4 model, given system and user messages as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mk9s7Cn209Au"
   },
   "outputs": [],
   "source": [
    "def generate_chat_response(system_content, user_content):\n",
    "    # Create two message dictionaries, one for the system and one for the user.\n",
    "    system = {'role': 'system', 'content': system_content}\n",
    "    user = {'role': 'user', 'content': user_content}\n",
    "\n",
    "    # Use OpenAI's ChatCompletion API to generate a response.\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[system, user],  # List of messages (system and user)\n",
    "        max_tokens=1200  # Set a limit on the number of tokens in the response\n",
    "    )\n",
    "\n",
    "    # Return the generated response.\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqynV95a0bg6"
   },
   "source": [
    "Let's craft a prompt to generate a simple python method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fF_6iPX0iuK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE2Yzdv0zdOw"
   },
   "source": [
    "Let's extract the code from the prompt response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zb7kWqheUWeq"
   },
   "outputs": [],
   "source": [
    "def extract_code(response_content):\n",
    "    # Define a regular expression pattern to match text between triple backticks (```)\n",
    "    pattern = r'```(.*?)```'\n",
    "\n",
    "    # Use re.findall to find all non-overlapping matches of the pattern in the input string\n",
    "    matches = re.findall(pattern, response_content, re.DOTALL)\n",
    "\n",
    "    # Remove the python keyword from in the code and Return the first match found\n",
    "    return matches[0].replace(\"python\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unC1WW7y08xg"
   },
   "source": [
    "Now let's execute the generated code and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBjGxmjhgkW1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OysV-evMv5n7"
   },
   "source": [
    "### Task 4 - Generate Python Code for Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5s8siqoxfvL"
   },
   "source": [
    "Defining `generate_code` helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3t-fTivvw9He"
   },
   "outputs": [],
   "source": [
    "def generate_code(system_content, user_content):\n",
    "    # Create two message dictionaries, one for the system and one for the user.\n",
    "    system = {'role': 'system', 'content': system_content}\n",
    "    user = {'role': 'user', 'content': user_content}\n",
    "\n",
    "    # Use OpenAI's ChatCompletion API to generate a response.\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[system, user],  # List of messages (system and user)\n",
    "        max_tokens=1200  # Set a limit on the number of tokens in the response\n",
    "    )\n",
    "\n",
    "    # Extract the response content\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    # Define a regular expression pattern to match text between triple backticks (```)\n",
    "    pattern = r'```(.*?)```'\n",
    "\n",
    "    # Use re.findall to find all non-overlapping matches of the pattern in the input string\n",
    "    matches = re.findall(pattern, response_content, re.DOTALL)\n",
    "\n",
    "    # Remove the python keyword from in the code and Return the first match found\n",
    "    return matches[0].replace(\"python\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDxhsA_EyoJF"
   },
   "source": [
    "Let's load product sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NiK75QPbyrgD",
    "outputId": "6e8d0047-bfef-4361-d94d-e39e50796ebe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDD0uSHXAiWo"
   },
   "source": [
    "Let's check the data types in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGS_hepG-THO",
    "outputId": "51307757-9c91-4585-b8ae-0dc387b1b623"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDU7R4ndAmjj"
   },
   "source": [
    "**Some Data Preparation**\n",
    "* Extract month name from the `date` column\n",
    "* Calculate profit per product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZonfVoUAltP",
    "outputId": "90e5cb49-54eb-4136-ca4d-e3480611563f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG2Ap0pNDOcK"
   },
   "source": [
    "Let's execute this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7q-JiKICDRIC",
    "outputId": "1a4cfa8a-3384-431e-b0da-ac0efc6b6a67"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs6frRFlD6WH"
   },
   "source": [
    "Lets calculate profit per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91f3QK2YDW_H",
    "outputId": "711bab8f-faa2-4730-be5e-d01b2f160ffd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVzzOwlTFT2A"
   },
   "source": [
    "Now let's execute this code and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ZS6IyKo4E5E1",
    "outputId": "548e1efe-ebd2-4edf-9db3-4ca39aa0f3c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z11i_5RpFsQp"
   },
   "source": [
    "### Task 5 - Generate Python Code for Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqlZldsFF8_k"
   },
   "source": [
    "Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FikByEF6FymM"
   },
   "outputs": [],
   "source": [
    "def generate_chat_response(system_content, user_content):\n",
    "    # Create two message dictionaries, one for the system and one for the user.\n",
    "    system = {'role': 'system', 'content': system_content}\n",
    "    user = {'role': 'user', 'content': user_content}\n",
    "\n",
    "    # Use OpenAI's ChatCompletion API to generate a response.\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[system, user],  # List of messages (system and user)\n",
    "        max_tokens=1200  # Set a limit on the number of tokens in the response\n",
    "    )\n",
    "\n",
    "    # Return the generated response.\n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_code(response_content):\n",
    "    # Define a regular expression pattern to match text between triple backticks (```)\n",
    "    pattern = r'```(.*?)```'\n",
    "\n",
    "    # Use re.findall to find all non-overlapping matches of the pattern in the input string\n",
    "    matches = re.findall(pattern, response_content, re.DOTALL)\n",
    "\n",
    "    # Remove the python keyword from in the code and Return the first match found\n",
    "    return matches[0].replace(\"python\", \"\")\n",
    "\n",
    "\n",
    "def generate_code_and_execute(user_content, execute=True):\n",
    "    # Defining system content for code generation\n",
    "    system_content = \"\"\"\n",
    "    You are a python code generator. You know pandas. You answer to every question with Python code.\n",
    "    You return python code wrapped in ``` delimiter. Import any neede python module. And you don't provide any elaborations.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate chat response\n",
    "    response = generate_chat_response(system_content, user_content)\n",
    "\n",
    "    # extract resonse content\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    # let's extract the code from the response content\n",
    "    code = extract_code(response_content)\n",
    "\n",
    "    if execute:\n",
    "        exec(code, globals())  # Execute the generated Python code if execute is set to True\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def update_code(code, user_content, execute = True):\n",
    "    # Defining system content for code update\n",
    "    system_content = f\"\"\"\n",
    "    You are a python code generator. You know pandas. You are given the following python method: {code}. Update the code based on the user content. Do not change the method name.\n",
    "    You return the updated python code wrapped in ``` delimiter. And you don't provide any elaborations.\n",
    "    \"\"\"\n",
    "    # generate chat response\n",
    "    response_content = generate_chat_response(system_content, user_content).choices[0].message.content\n",
    "\n",
    "    # extracting the code\n",
    "    new_code = extract_code(response_content)\n",
    "\n",
    "    if execute:\n",
    "        exec(new_code, globals())  # Execute the generated Python code if execute is set to True\n",
    "\n",
    "    return new_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTqZ35pjtnRx"
   },
   "source": [
    "Let's check our data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uyxBLUy8svbK",
    "outputId": "10e18303-b448-4586-a817-1b5deb7d0c6d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26FZODafwCey"
   },
   "source": [
    "\n",
    "**Question 1-** How does the daily `average` profit change over time?\n",
    "\n",
    "Steps to answer this question:\n",
    "\n",
    "1. Aggregate our data to have average `Product_Profit` per day.\n",
    "2. Draw a line chart to visualize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAHyXbfOvX3J",
    "outputId": "bb880ced-8df1-4355-9c50-638be4f0fd90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpe6jn3B8uhE"
   },
   "source": [
    "Let's use the generated method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "hY9QhMIV5yea",
    "outputId": "adf686e2-f32e-48fa-b73c-6f6145377b33"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAU20IviGGb4"
   },
   "source": [
    "What if we want to update the generated visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8ojRLz8zem5",
    "outputId": "014039c8-4e99-4f20-9c67-57f43cd894f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucJEB92I_Ltz"
   },
   "source": [
    "Running the new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "uFnsmn285Vpe",
    "outputId": "3bb23e7d-f68e-43ca-a952-0b961e5edf9e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGk0ftFur9xs"
   },
   "source": [
    "**Execrise:** Question 2 - Create a barchart visualization to show total average profit per Product Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXK-mk2W5ftU",
    "outputId": "bc866470-7f8f-468d-faed-767a5ffe6546"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "XaeApPxnuThL",
    "outputId": "0952a615-865a-45b7-9be0-c137397a5f4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfu-fTgIuZlF"
   },
   "source": [
    "Updating the code to have bar chart with different bar color per product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "y36xxUknueeE",
    "outputId": "d3017039-f8eb-4ea3-a390-7e888dfd8444"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3smjNBfzJ8W"
   },
   "source": [
    "### Task 6 - Create Visualizations using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7J5WHcDHbTB"
   },
   "source": [
    "Let's checkout the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TSnc88aP6mo3",
    "outputId": "b8ab2b64-c5b2-45d0-88c2-c88a9c3027ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r1Py-TszzP7"
   },
   "source": [
    "**Question 3** - Which product has the highest total sold items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "i3TSRVbGzusN",
    "outputId": "a24f15e7-99ae-46c0-d88e-92c4a04ce4de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8UPA-5_7wXG"
   },
   "source": [
    "Let's make it a horizontal bar chart and highlight the prodcut with the highest number of sold items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "Uwy_OTDm7e6I",
    "outputId": "a6f38585-3fa9-4e39-c0ab-d711c71a1bd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZlZamiGhutkW",
    "2sF80TRaQTlo",
    "WHNLz8cmXQ8x",
    "OysV-evMv5n7",
    "BDU7R4ndAmjj",
    "z11i_5RpFsQp",
    "x3smjNBfzJ8W"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
